{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "error",
     "timestamp": 1743620315360,
     "user": {
      "displayName": "ryan punamiya",
      "userId": "01065383722440132219"
     },
     "user_tz": 240
    },
    "id": "VrX4VTl5pYNq",
    "outputId": "c09c5b06-4ba3-4dca-a026-1db92e4d0208",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import collections\n",
    "import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# env import\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "L5E-nR6ornyg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@markdown ### **Environment**\n",
    "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
    "#@markdown And it's subclass `PushTImageEnv`.\n",
    "#@markdown\n",
    "#@markdown **Goal**: push the gray T-block into the green area.\n",
    "#@markdown\n",
    "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
    "\n",
    "\n",
    "positive_y_is_up: bool = False\n",
    "\"\"\"Make increasing values of y point upwards.\n",
    "\n",
    "When True::\n",
    "\n",
    "    y\n",
    "    ^\n",
    "    |      . (3, 3)\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    +------ > x\n",
    "\n",
    "When False::\n",
    "\n",
    "    +------ > x\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    |      . (3, 3)\n",
    "    v\n",
    "    y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
    "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
    "    local coordinates.\n",
    "\n",
    "    Note that in case positive_y_is_up is False, this function wont actually do\n",
    "    anything except converting the point to integers.\n",
    "    \"\"\"\n",
    "    if positive_y_is_up:\n",
    "        return round(p[0]), surface.get_height() - round(p[1])\n",
    "    else:\n",
    "        return round(p[0]), round(p[1])\n",
    "\n",
    "\n",
    "def light_color(color: SpaceDebugColor):\n",
    "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
    "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
    "    return color\n",
    "\n",
    "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
    "    def __init__(self, surface: pygame.Surface) -> None:\n",
    "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
    "\n",
    "        Typical usage::\n",
    "\n",
    "        >>> import pymunk\n",
    "        >>> surface = pygame.Surface((10,10))\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        You can control the color of a shape by setting shape.color to the color\n",
    "        you want it drawn in::\n",
    "\n",
    "        >>> c = pymunk.Circle(None, 10)\n",
    "        >>> c.color = pygame.Color(\"pink\")\n",
    "\n",
    "        See pygame_util.demo.py for a full example\n",
    "\n",
    "        Since pygame uses a coordiante system where y points down (in contrast\n",
    "        to many other cases), you either have to make the physics simulation\n",
    "        with Pymunk also behave in that way, or flip everything when you draw.\n",
    "\n",
    "        The easiest is probably to just make the simulation behave the same\n",
    "        way as Pygame does. In that way all coordinates used are in the same\n",
    "        orientation and easy to reason about::\n",
    "\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> space.gravity = (0, -1000)\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        To flip the drawing its possible to set the module property\n",
    "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
    "        the simulation upside down before drawing::\n",
    "\n",
    "        >>> positive_y_is_up = True\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0)\n",
    "        >>> # Body will be position in bottom left corner\n",
    "\n",
    "        :Parameters:\n",
    "                surface : pygame.Surface\n",
    "                    Surface that the objects will be drawn on\n",
    "        \"\"\"\n",
    "        self.surface = surface\n",
    "        super(DrawOptions, self).__init__()\n",
    "\n",
    "    def draw_circle(\n",
    "        self,\n",
    "        pos: Vec2d,\n",
    "        angle: float,\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "\n",
    "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
    "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
    "\n",
    "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
    "        p2 = to_pygame(circle_edge, self.surface)\n",
    "        line_r = 2 if radius > 20 else 1\n",
    "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
    "\n",
    "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
    "\n",
    "    def draw_fat_segment(\n",
    "        self,\n",
    "        a: Tuple[float, float],\n",
    "        b: Tuple[float, float],\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        r = round(max(1, radius * 2))\n",
    "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
    "        if r > 2:\n",
    "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
    "            if orthog[0] == 0 and orthog[1] == 0:\n",
    "                return\n",
    "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
    "            orthog[0] = round(orthog[0] * scale)\n",
    "            orthog[1] = round(orthog[1] * scale)\n",
    "            points = [\n",
    "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
    "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
    "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
    "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
    "            ]\n",
    "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p1[0]), round(p1[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p2[0]), round(p2[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "\n",
    "    def draw_polygon(\n",
    "        self,\n",
    "        verts: Sequence[Tuple[float, float]],\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        ps = [to_pygame(v, self.surface) for v in verts]\n",
    "        ps += [ps[0]]\n",
    "\n",
    "        radius = 2\n",
    "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
    "\n",
    "        if radius > 0:\n",
    "            for i in range(len(verts)):\n",
    "                a = verts[i]\n",
    "                b = verts[(i + 1) % len(verts)]\n",
    "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
    "\n",
    "    def draw_dot(\n",
    "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
    "\n",
    "def pymunk_to_shapely(body, shapes):\n",
    "    geoms = list()\n",
    "    for shape in shapes:\n",
    "        if isinstance(shape, pymunk.shapes.Poly):\n",
    "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
    "            verts += [verts[0]]\n",
    "            geoms.append(sg.Polygon(verts))\n",
    "        else:\n",
    "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
    "    geom = sg.MultiPolygon(geoms)\n",
    "    return geom\n",
    "\n",
    "# env\n",
    "class PushTEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "    reward_range = (0., 1.)\n",
    "\n",
    "    def __init__(self,\n",
    "            legacy=False,\n",
    "            block_cog=None, damping=None,\n",
    "            render_action=True,\n",
    "            render_size=96,\n",
    "            reset_to_state=None\n",
    "        ):\n",
    "        self._seed = None\n",
    "        self.seed()\n",
    "        self.window_size = ws = 512  # The size of the PyGame window\n",
    "        self.render_size = render_size\n",
    "        self.sim_hz = 100\n",
    "        # Local controller params.\n",
    "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
    "        self.control_hz = self.metadata['video.frames_per_second']\n",
    "        # legcay set_state for data compatiblity\n",
    "        self.legacy = legacy\n",
    "\n",
    "        # agent_pos, block_pos, block_angle\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
    "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
    "            shape=(5,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # positional goal for agent\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0,0], dtype=np.float64),\n",
    "            high=np.array([ws,ws], dtype=np.float64),\n",
    "            shape=(2,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        self.block_cog = block_cog\n",
    "        self.damping = damping\n",
    "        self.render_action = render_action\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.screen = None\n",
    "\n",
    "        self.space = None\n",
    "        self.teleop = None\n",
    "        self.render_buffer = None\n",
    "        self.latest_action = None\n",
    "        self.reset_to_state = reset_to_state\n",
    "\n",
    "    def reset(self):\n",
    "        seed = self._seed\n",
    "        self._setup()\n",
    "        if self.block_cog is not None:\n",
    "            self.block.center_of_gravity = self.block_cog\n",
    "        if self.damping is not None:\n",
    "            self.space.damping = self.damping\n",
    "\n",
    "        # use legacy RandomState for compatiblity\n",
    "        state = self.reset_to_state\n",
    "        if state is None:\n",
    "            rs = np.random.RandomState(seed=seed)\n",
    "            state = np.array([\n",
    "                rs.randint(50, 450), rs.randint(50, 450),\n",
    "                rs.randint(100, 400), rs.randint(100, 400),\n",
    "                rs.randn() * 2 * np.pi - np.pi\n",
    "                ])\n",
    "        self._set_state(state)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        dt = 1.0 / self.sim_hz\n",
    "        self.n_contact_points = 0\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        if action is not None:\n",
    "            self.latest_action = action\n",
    "            for i in range(n_steps):\n",
    "                # Step PD control.\n",
    "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
    "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
    "                self.agent.velocity += acceleration * dt\n",
    "\n",
    "                # Step physics.\n",
    "                self.space.step(dt)\n",
    "\n",
    "        # compute reward\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
    "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
    "\n",
    "        intersection_area = goal_geom.intersection(block_geom).area\n",
    "        goal_area = goal_geom.area\n",
    "        coverage = intersection_area / goal_area\n",
    "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
    "        done = coverage > self.success_threshold\n",
    "        terminated = done\n",
    "        truncated = done\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self._render_frame(mode)\n",
    "\n",
    "    def teleop_agent(self):\n",
    "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
    "        def act(obs):\n",
    "            act = None\n",
    "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
    "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
    "                self.teleop = True\n",
    "                act = mouse_position\n",
    "            return act\n",
    "        return TeleopAgent(act)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.array(\n",
    "            tuple(self.agent.position) \\\n",
    "            + tuple(self.block.position) \\\n",
    "            + (self.block.angle % (2 * np.pi),))\n",
    "        return obs\n",
    "\n",
    "    def _get_goal_pose_body(self, pose):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        # preserving the legacy assignment order for compatibility\n",
    "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
    "        body.position = pose[:2].tolist()\n",
    "        body.angle = pose[2]\n",
    "        return body\n",
    "\n",
    "    def _get_info(self):\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
    "        info = {\n",
    "            'pos_agent': np.array(self.agent.position),\n",
    "            'vel_agent': np.array(self.agent.velocity),\n",
    "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
    "            'goal_pose': self.goal_pose,\n",
    "            'n_contacts': n_contact_points_per_step}\n",
    "        return info\n",
    "\n",
    "    def _render_frame(self, mode):\n",
    "\n",
    "        if self.window is None and mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        self.screen = canvas\n",
    "\n",
    "        draw_options = DrawOptions(canvas)\n",
    "\n",
    "        # Draw goal pose.\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        for shape in self.block.shapes:\n",
    "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
    "            goal_points += [goal_points[0]]\n",
    "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
    "\n",
    "        # Draw agent and block.\n",
    "        self.space.debug_draw(draw_options)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # the clock is aleady ticked during in step for \"human\"\n",
    "\n",
    "\n",
    "        img = np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
    "        if self.render_action:\n",
    "            if self.render_action and (self.latest_action is not None):\n",
    "                action = np.array(self.latest_action)\n",
    "                coord = (action / 512 * 96).astype(np.int32)\n",
    "                marker_size = int(8/96*self.render_size)\n",
    "                thickness = int(1/96*self.render_size)\n",
    "                cv2.drawMarker(img, coord,\n",
    "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
    "                    markerSize=marker_size, thickness=thickness)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0,25536)\n",
    "        self._seed = seed\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def _handle_collision(self, arbiter, space, data):\n",
    "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = state.tolist()\n",
    "        pos_agent = state[:2]\n",
    "        pos_block = state[2:4]\n",
    "        rot_block = state[4]\n",
    "        self.agent.position = pos_agent\n",
    "        # setting angle rotates with respect to center of mass\n",
    "        # therefore will modify the geometric position\n",
    "        # if not the same as CoM\n",
    "        # therefore should be modified first.\n",
    "        if self.legacy:\n",
    "            # for compatiblity with legacy data\n",
    "            self.block.position = pos_block\n",
    "            self.block.angle = rot_block\n",
    "        else:\n",
    "            self.block.angle = rot_block\n",
    "            self.block.position = pos_block\n",
    "\n",
    "        # Run physics to take effect\n",
    "        self.space.step(1.0 / self.sim_hz)\n",
    "\n",
    "    def _set_state_local(self, state_local):\n",
    "        agent_pos_local = state_local[:2]\n",
    "        block_pose_local = state_local[2:]\n",
    "        tf_img_obj = st.AffineTransform(\n",
    "            translation=self.goal_pose[:2],\n",
    "            rotation=self.goal_pose[2])\n",
    "        tf_obj_new = st.AffineTransform(\n",
    "            translation=block_pose_local[:2],\n",
    "            rotation=block_pose_local[2]\n",
    "        )\n",
    "        tf_img_new = st.AffineTransform(\n",
    "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
    "        )\n",
    "        agent_pos_new = tf_img_new(agent_pos_local)\n",
    "        new_state = np.array(\n",
    "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
    "                + [tf_img_new.rotation])\n",
    "        self._set_state(new_state)\n",
    "        return new_state\n",
    "\n",
    "    def _setup(self):\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = 0, 0\n",
    "        self.space.damping = 0\n",
    "        self.teleop = False\n",
    "        self.render_buffer = list()\n",
    "\n",
    "        # Add walls.\n",
    "        walls = [\n",
    "            self._add_segment((5, 506), (5, 5), 2),\n",
    "            self._add_segment((5, 5), (506, 5), 2),\n",
    "            self._add_segment((506, 5), (506, 506), 2),\n",
    "            self._add_segment((5, 506), (506, 506), 2)\n",
    "        ]\n",
    "        self.space.add(*walls)\n",
    "\n",
    "        # Add agent, block, and goal zone.\n",
    "        self.agent = self.add_circle((256, 400), 15)\n",
    "        self.block = self.add_tee((256, 300), 0)\n",
    "        self.goal_color = pygame.Color('LightGreen')\n",
    "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
    "        \n",
    "    \n",
    "\n",
    "        # Add collision handeling\n",
    "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
    "        self.collision_handeler.post_solve = self._handle_collision\n",
    "        self.n_contact_points = 0\n",
    "\n",
    "        self.max_score = 50 * 100\n",
    "        self.success_threshold = 0.95    # 95% coverage.\n",
    "    \n",
    "    def add_triangle(self, position, size=30, color=\"LightCoral\"):\n",
    "        h = size * np.sqrt(3) / 2  # height of equilateral triangle\n",
    "        vertices = [\n",
    "            (0, -2*h/3),            # bottom point\n",
    "            (-size/2, h/3),         # top-left\n",
    "            (size/2, h/3)           # top-right\n",
    "        ]\n",
    "        \n",
    "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
    "        body.position = position\n",
    "        body.friction = 0.8 # slight diff friction\n",
    "        \n",
    "        shape = pymunk.Poly(body, vertices)\n",
    "        shape.color = pygame.Color(color)\n",
    "        \n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "    \n",
    "    def _add_segment(self, a, b, radius):\n",
    "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
    "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
    "        return shape\n",
    "\n",
    "    def add_circle(self, position, radius):\n",
    "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
    "        body.position = position\n",
    "        body.friction = 1\n",
    "        shape = pymunk.Circle(body, radius)\n",
    "        shape.color = pygame.Color('RoyalBlue')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_box(self, position, height, width):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        body.position = position\n",
    "        shape = pymunk.Poly.create_box(body, (height, width))\n",
    "        shape.color = pygame.Color('LightSlateGray')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
    "        mass = 1\n",
    "        length = 4\n",
    "        vertices1 = [(-length*scale/2, scale),\n",
    "                                 ( length*scale/2, scale),\n",
    "                                 ( length*scale/2, 0),\n",
    "                                 (-length*scale/2, 0)]\n",
    "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        vertices2 = [(-scale/2, scale),\n",
    "                                 (-scale/2, length*scale),\n",
    "                                 ( scale/2, length*scale),\n",
    "                                 ( scale/2, scale)]\n",
    "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
    "        shape1 = pymunk.Poly(body, vertices1)\n",
    "        shape2 = pymunk.Poly(body, vertices2)\n",
    "        shape1.color = pygame.Color(color)\n",
    "        shape2.color = pygame.Color(color)\n",
    "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
    "        body.position = position\n",
    "        body.angle = angle\n",
    "        body.friction = 1\n",
    "        self.space.add(body, shape1, shape2)\n",
    "        return body\n",
    "\n",
    "\n",
    "class PushTImageEnv(PushTEnv):\n",
    "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "\n",
    "    def __init__(self,\n",
    "            legacy=False,\n",
    "            block_cog=None,\n",
    "            damping=None,\n",
    "            render_size=96):\n",
    "        super().__init__(\n",
    "            legacy=legacy,\n",
    "            block_cog=block_cog,\n",
    "            damping=damping,\n",
    "            render_size=render_size,\n",
    "            render_action=False)\n",
    "        ws = self.window_size\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'image': spaces.Box(\n",
    "                low=0,\n",
    "                high=1,\n",
    "                shape=(3,render_size,render_size),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_pos': spaces.Box(\n",
    "                low=0,\n",
    "                high=ws,\n",
    "                shape=(2,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        self.render_cache = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        img = super()._render_frame(mode='rgb_array')\n",
    "\n",
    "        agent_pos = np.array(self.agent.position)\n",
    "        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n",
    "        obs = {\n",
    "            'image': img_obs,\n",
    "            'agent_pos': agent_pos\n",
    "        }\n",
    "\n",
    "        # draw action\n",
    "        if self.latest_action is not None:\n",
    "            action = np.array(self.latest_action)\n",
    "            coord = (action / 512 * 96).astype(np.int32)\n",
    "            marker_size = int(8/96*self.render_size)\n",
    "            thickness = int(1/96*self.render_size)\n",
    "            cv2.drawMarker(img, coord,\n",
    "                color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
    "                markerSize=marker_size, thickness=thickness)\n",
    "        self.render_cache = img\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode):\n",
    "        assert mode == 'rgb_array'\n",
    "\n",
    "        if self.render_cache is None:\n",
    "            self._get_obs()\n",
    "\n",
    "        return self.render_cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PushTEnv2(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "    reward_range = (0., 1.)\n",
    "\n",
    "    def __init__(self,\n",
    "            legacy=False,\n",
    "            block_cog=None, damping=None,\n",
    "            render_action=True,\n",
    "            render_size=96,\n",
    "            reset_to_state=None\n",
    "        ):\n",
    "        self._seed = None\n",
    "        self.seed()\n",
    "        self.window_size = ws = 512  # The size of the PyGame window\n",
    "        self.render_size = render_size\n",
    "        self.sim_hz = 100\n",
    "        # Local controller params.\n",
    "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
    "        self.control_hz = self.metadata['video.frames_per_second']\n",
    "        # legcay set_state for data compatiblity\n",
    "        self.legacy = legacy\n",
    "\n",
    "        # agent_pos, block_pos, block_angle\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
    "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
    "            shape=(5,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # positional goal for agent\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0,0], dtype=np.float64),\n",
    "            high=np.array([ws,ws], dtype=np.float64),\n",
    "            shape=(2,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        self.block_cog = block_cog\n",
    "        self.damping = damping\n",
    "        self.render_action = render_action\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.screen = None\n",
    "\n",
    "        self.space = None\n",
    "        self.teleop = None\n",
    "        self.render_buffer = None\n",
    "        self.latest_action = None\n",
    "        self.reset_to_state = reset_to_state\n",
    "\n",
    "    def reset(self):\n",
    "        seed = self._seed\n",
    "        self._setup()\n",
    "        if self.block_cog is not None:\n",
    "            self.block.center_of_gravity = self.block_cog\n",
    "        if self.damping is not None:\n",
    "            self.space.damping = self.damping\n",
    "\n",
    "        # use legacy RandomState for compatiblity\n",
    "        state = self.reset_to_state\n",
    "        if state is None:\n",
    "            rs = np.random.RandomState(seed=seed)\n",
    "            state = np.array([\n",
    "                rs.randint(50, 450), rs.randint(50, 450),\n",
    "                rs.randint(100, 400), rs.randint(100, 400),\n",
    "                rs.randn() * 2 * np.pi - np.pi\n",
    "                ])\n",
    "        self._set_state(state)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        dt = 1.0 / self.sim_hz\n",
    "        self.n_contact_points = 0\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        if action is not None:\n",
    "            self.latest_action = action\n",
    "            for i in range(n_steps):\n",
    "                # Step PD control.\n",
    "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
    "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
    "                self.agent.velocity += acceleration * dt\n",
    "\n",
    "                # Step physics.\n",
    "                self.space.step(dt)\n",
    "\n",
    "        # compute reward\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
    "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
    "\n",
    "        intersection_area = goal_geom.intersection(block_geom).area\n",
    "        goal_area = goal_geom.area\n",
    "        coverage = intersection_area / goal_area\n",
    "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
    "        done = coverage > self.success_threshold\n",
    "        terminated = done\n",
    "        truncated = done\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self._render_frame(mode)\n",
    "\n",
    "    def teleop_agent(self):\n",
    "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
    "        def act(obs):\n",
    "            act = None\n",
    "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
    "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
    "                self.teleop = True\n",
    "                act = mouse_position\n",
    "            return act\n",
    "        return TeleopAgent(act)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.array(\n",
    "            tuple(self.agent.position) \\\n",
    "            + tuple(self.block.position) \\\n",
    "            + (self.block.angle % (2 * np.pi),))\n",
    "        return obs\n",
    "\n",
    "    def _get_goal_pose_body(self, pose):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        # preserving the legacy assignment order for compatibility\n",
    "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
    "        body.position = pose[:2].tolist()\n",
    "        body.angle = pose[2]\n",
    "        return body\n",
    "\n",
    "    def _get_info(self):\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
    "        info = {\n",
    "            'pos_agent': np.array(self.agent.position),\n",
    "            'vel_agent': np.array(self.agent.velocity),\n",
    "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
    "            'goal_pose': self.goal_pose,\n",
    "            'n_contacts': n_contact_points_per_step}\n",
    "        return info\n",
    "\n",
    "    def _render_frame(self, mode):\n",
    "\n",
    "        if self.window is None and mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((186, 85, 211))\n",
    "        self.screen = canvas\n",
    "\n",
    "        draw_options = DrawOptions(canvas)\n",
    "\n",
    "        # Draw goal pose.\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        for shape in self.block.shapes:\n",
    "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
    "            goal_points += [goal_points[0]]\n",
    "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
    "\n",
    "        # Draw agent and block.\n",
    "        self.space.debug_draw(draw_options)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # the clock is aleady ticked during in step for \"human\"\n",
    "\n",
    "\n",
    "        img = np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
    "        if self.render_action:\n",
    "            if self.render_action and (self.latest_action is not None):\n",
    "                action = np.array(self.latest_action)\n",
    "                coord = (action / 512 * 96).astype(np.int32)\n",
    "                marker_size = int(8/96*self.render_size)\n",
    "                thickness = int(1/96*self.render_size)\n",
    "                cv2.drawMarker(img, coord,\n",
    "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
    "                    markerSize=marker_size, thickness=thickness)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0,25536)\n",
    "        self._seed = seed\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def _handle_collision(self, arbiter, space, data):\n",
    "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = state.tolist()\n",
    "        pos_agent = state[:2]\n",
    "        pos_block = state[2:4]\n",
    "        rot_block = state[4]\n",
    "        \n",
    "        pos_agent = self.rotate90_clockwise(state[:2])\n",
    "        pos_block = self.rotate90_clockwise(state[2:4])\n",
    "        rot_block = state[4] - np.pi / 2  # subtract 90 degrees in radians\n",
    "\n",
    "        self.agent.position = pos_agent\n",
    "        # setting angle rotates with respect to center of mass\n",
    "        # therefore will modify the geometric position\n",
    "        # if not the same as CoM\n",
    "        # therefore should be modified first.\n",
    "        if self.legacy:\n",
    "            # for compatiblity with legacy data\n",
    "            self.block.position = pos_block\n",
    "            self.block.angle = rot_block\n",
    "        else:\n",
    "            self.block.angle = rot_block\n",
    "            self.block.position = pos_block\n",
    "\n",
    "        # Run physics to take effect\n",
    "        self.space.step(1.0 / self.sim_hz)\n",
    "\n",
    "    def _set_state_local(self, state_local):\n",
    "        agent_pos_local = state_local[:2]\n",
    "        block_pose_local = state_local[2:]\n",
    "        tf_img_obj = st.AffineTransform(\n",
    "            translation=self.goal_pose[:2],\n",
    "            rotation=self.goal_pose[2])\n",
    "        tf_obj_new = st.AffineTransform(\n",
    "            translation=block_pose_local[:2],\n",
    "            rotation=block_pose_local[2]\n",
    "        )\n",
    "        tf_img_new = st.AffineTransform(\n",
    "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
    "        )\n",
    "        agent_pos_new = tf_img_new(agent_pos_local)\n",
    "        new_state = np.array(\n",
    "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
    "                + [tf_img_new.rotation])\n",
    "        self._set_state(new_state)\n",
    "        return new_state\n",
    "\n",
    "    def _setup(self):\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = 0, 0\n",
    "        self.space.damping = 0\n",
    "        self.teleop = False\n",
    "        self.render_buffer = list()\n",
    "\n",
    "        # Add walls.\n",
    "        walls = [\n",
    "            self._add_segment((5, 506), (5, 5), 2),\n",
    "            self._add_segment((5, 5), (506, 5), 2),\n",
    "            self._add_segment((506, 5), (506, 506), 2),\n",
    "            self._add_segment((5, 506), (506, 506), 2)\n",
    "        ]\n",
    "        self.space.add(*walls)\n",
    "\n",
    "        # Add agent, block, and goal zone.\n",
    "        self.agent = self.add_triangle((256, 400), 40)\n",
    "        self.block = self.add_tee((256, 300), 0)\n",
    "        self.goal_color = pygame.Color('LightGreen')\n",
    "        goal_pose = np.array([256, 256, np.pi/4])\n",
    "\n",
    "        # Rotate position\n",
    "        goal_pose[:2] = self.rotate90_clockwise(goal_pose[:2])\n",
    "\n",
    "        # Rotate orientation\n",
    "        goal_pose[2] -= np.pi / 2\n",
    "\n",
    "        self.goal_pose = goal_pose\n",
    "\n",
    "        # Add collision handeling\n",
    "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
    "        self.collision_handeler.post_solve = self._handle_collision\n",
    "        self.n_contact_points = 0\n",
    "\n",
    "        self.max_score = 50 * 100\n",
    "        self.success_threshold = 0.95    # 95% coverage.\n",
    "    \n",
    "    def add_triangle(self, position, size=30, color=\"LightCoral\"):\n",
    "        h = size * np.sqrt(3) / 2  # height of equilateral triangle\n",
    "        vertices = [\n",
    "            (0, -2*h/3),            # bottom point\n",
    "            (-size/2, h/3),         # top-left\n",
    "            (size/2, h/3)           # top-right\n",
    "        ]\n",
    "        \n",
    "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
    "        body.position = position\n",
    "        body.friction = 0.8 # slight diff friction\n",
    "        \n",
    "        shape = pymunk.Poly(body, vertices)\n",
    "        shape.color = pygame.Color(color)\n",
    "        \n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "    \n",
    "    def _add_segment(self, a, b, radius):\n",
    "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
    "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
    "        return shape\n",
    "\n",
    "    def add_circle(self, position, radius):\n",
    "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
    "        body.position = position\n",
    "        body.friction = 1\n",
    "        shape = pymunk.Circle(body, radius)\n",
    "        shape.color = pygame.Color('RoyalBlue')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_box(self, position, height, width):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        body.position = position\n",
    "        shape = pymunk.Poly.create_box(body, (height, width))\n",
    "        shape.color = pygame.Color('LightSlateGray')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
    "        mass = 1\n",
    "        length = 4\n",
    "        vertices1 = [(-length*scale/2, scale),\n",
    "                                 ( length*scale/2, scale),\n",
    "                                 ( length*scale/2, 0),\n",
    "                                 (-length*scale/2, 0)]\n",
    "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        vertices2 = [(-scale/2, scale),\n",
    "                                 (-scale/2, length*scale),\n",
    "                                 ( scale/2, length*scale),\n",
    "                                 ( scale/2, scale)]\n",
    "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
    "        shape1 = pymunk.Poly(body, vertices1)\n",
    "        shape2 = pymunk.Poly(body, vertices2)\n",
    "        shape1.color = pygame.Color(color)\n",
    "        shape2.color = pygame.Color(color)\n",
    "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
    "        body.position = position\n",
    "        body.angle = angle\n",
    "        body.friction = 1\n",
    "        self.space.add(body, shape1, shape2)\n",
    "        return body\n",
    "\n",
    "    def rotate90_clockwise(self, pos, center=(256, 256)):\n",
    "        x0, y0 = pos[0] - center[0], pos[1] - center[1]\n",
    "        x1, y1 = y0, -x0\n",
    "        return [x1 + center[0], y1 + center[1]]\n",
    "    \n",
    "    def rotate_keypoints(self, kps, center=(256, 256)):\n",
    "        kps_rot = kps.copy()\n",
    "        for i in range(len(kps)):\n",
    "            x, y = kps[i]\n",
    "            x0, y0 = x - center[0], y - center[1]\n",
    "            x1, y1 = y0, -x0\n",
    "            kps_rot[i] = [x1 + center[0], y1 + center[1]]\n",
    "        return kps_rot\n",
    "    \n",
    "\n",
    "class PushTImageEnv2(PushTEnv2):\n",
    "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "\n",
    "    def __init__(self,\n",
    "            legacy=False,\n",
    "            block_cog=None,\n",
    "            damping=None,\n",
    "            render_size=96):\n",
    "        super().__init__(\n",
    "            legacy=legacy,\n",
    "            block_cog=block_cog,\n",
    "            damping=damping,\n",
    "            render_size=render_size,\n",
    "            render_action=False)\n",
    "        ws = self.window_size\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'image': spaces.Box(\n",
    "                low=0,\n",
    "                high=1,\n",
    "                shape=(3,render_size,render_size),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_pos': spaces.Box(\n",
    "                low=0,\n",
    "                high=ws,\n",
    "                shape=(2,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        self.render_cache = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        img = super()._render_frame(mode='rgb_array')\n",
    "\n",
    "        agent_pos = np.array(self.agent.position)\n",
    "        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n",
    "        obs = {\n",
    "            'image': img_obs,\n",
    "            'agent_pos': agent_pos\n",
    "        }\n",
    "\n",
    "        # draw action\n",
    "        if self.latest_action is not None:\n",
    "            action = np.array(self.latest_action)\n",
    "            coord = (action / 512 * 96).astype(np.int32)\n",
    "            marker_size = int(8/96*self.render_size)\n",
    "            thickness = int(1/96*self.render_size)\n",
    "            cv2.drawMarker(img, coord,\n",
    "                color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
    "                markerSize=marker_size, thickness=thickness)\n",
    "        self.render_cache = img\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode):\n",
    "        assert mode == 'rgb_array'\n",
    "\n",
    "        if self.render_cache is None:\n",
    "            self._get_obs()\n",
    "\n",
    "        return self.render_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OknH8Qfqrtc9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs['image'].shape: (3, 96, 96) float32, [0,1]\n",
      "obs['agent_pos'].shape: (2,) float32, [0,512]\n",
      "action.shape:  (2,) float32, [0,512]\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Env Demo**\n",
    "#@markdown Standard Gym Env (0.21.0 API)\n",
    "\n",
    "# 0. create env object\n",
    "env = PushTImageEnv()\n",
    "\n",
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "env.seed(1000)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 3. 2D positional action space [0,512]\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n",
    "    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n",
    "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "vHepJOFBucwg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@markdown ### **Dataset**\n",
    "#@markdown\n",
    "#@markdown Defines `PushTImageDataset` and helper functions\n",
    "#@markdown\n",
    "#@markdown The dataset class\n",
    "#@markdown - Load data ((image, agent_pos), action) from a zarr storage\n",
    "#@markdown - Normalizes each dimension of agent_pos and action to [-1,1]\n",
    "#@markdown - Returns\n",
    "#@markdown  - All possible segments with length `pred_horizon`\n",
    "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
    "#@markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n",
    "#@markdown  - key `agent_pos`: shape (obs_hoirzon, 2)\n",
    "#@markdown  - key `action`: shape (pred_horizon, 2)\n",
    "\n",
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "def get_joint_stats(dataset_paths):\n",
    "    all_agent_pos = []\n",
    "    all_actions = []\n",
    "\n",
    "    for path in dataset_paths:\n",
    "        z = zarr.open(path, 'r')\n",
    "        all_agent_pos.append(z['data']['state'][:, :2])\n",
    "        all_actions.append(z['data']['action'][:])\n",
    "\n",
    "    all_agent_pos = np.concatenate(all_agent_pos, axis=0)\n",
    "    all_actions = np.concatenate(all_actions, axis=0)\n",
    "\n",
    "    def get_data_stats(data):\n",
    "        data = data.reshape(-1, data.shape[-1])\n",
    "        return {\n",
    "            'min': np.min(data, axis=0),\n",
    "            'max': np.max(data, axis=0)\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'agent_pos': get_data_stats(all_agent_pos),\n",
    "        'action': get_data_stats(all_actions),\n",
    "    }\n",
    "    \n",
    "class PushTImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path: str,\n",
    "                 pred_horizon: int,\n",
    "                 obs_horizon: int,\n",
    "                 action_horizon: int,\n",
    "                 shared_stats=None):\n",
    "\n",
    "        dataset_root = zarr.open(dataset_path, 'r')\n",
    "        train_image_data = dataset_root['data']['img'][:]\n",
    "        train_image_data = np.moveaxis(train_image_data, -1, 1)\n",
    "\n",
    "        train_data = {\n",
    "            'agent_pos': dataset_root['data']['state'][:, :2],\n",
    "            'action': dataset_root['data']['action'][:]\n",
    "        }\n",
    "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
    "\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=action_horizon-1\n",
    "        )\n",
    "\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = shared_stats[key] if shared_stats else get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        normalized_train_data['image'] = train_image_data\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "        nsample['image'] = nsample['image'][:self.obs_horizon, :]\n",
    "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon, :]\n",
    "        return nsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class MultiPushTImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_paths, pred_horizon, obs_horizon, action_horizon):\n",
    "        self.shared_stats = get_joint_stats(dataset_paths)\n",
    "\n",
    "        self.datasets = [\n",
    "            PushTImageDataset(\n",
    "                path,\n",
    "                pred_horizon=pred_horizon,\n",
    "                obs_horizon=obs_horizon,\n",
    "                action_horizon=action_horizon,\n",
    "                shared_stats=self.shared_stats\n",
    "            )\n",
    "            for path in dataset_paths\n",
    "        ]\n",
    "        self.all_samples = sum(len(d) for d in self.datasets)\n",
    "        self.dataset_offsets = np.cumsum([0] + [len(d) for d in self.datasets[:-1]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.all_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx = np.searchsorted(self.dataset_offsets, idx, side='right') - 1\n",
    "        sample_idx = idx - self.dataset_offsets[dataset_idx]\n",
    "        return self.datasets[dataset_idx][sample_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred_horizon = 16\n",
    "obs_horizon = 1\n",
    "action_horizon = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  1307\n",
      "batch['image'].shape: torch.Size([32, 1, 3, 96, 96])\n",
      "batch['agent_pos'].shape: torch.Size([32, 1, 2])\n",
      "batch['action'].shape torch.Size([32, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_paths = ['./data/range_A_base.zarr', './data/range_A_perturb.zarr', './data/range_B_base.zarr']\n",
    "# Parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 1\n",
    "action_horizon = 8\n",
    "\n",
    "# Create Multi-Zarr dataset\n",
    "dataset = MultiPushTImageDataset(\n",
    "    dataset_paths=dataset_paths,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon\n",
    ")\n",
    "\n",
    "# Save training data statistics (min, max) for each dataset\n",
    "stats = {path: ds.stats for path, ds in zip(dataset_paths, dataset.datasets)}\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Length: \", len(dataloader))\n",
    "print(\"batch['image'].shape:\", batch['image'].shape)\n",
    "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "X-XRB_g3vsgf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yXq4r744aMh1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@markdown ### **Vision Encoder**\n",
    "#@markdown\n",
    "#@markdown Defines helper functions:\n",
    "#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
    "#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
    "\n",
    "def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n",
    "    \"\"\"\n",
    "    name: resnet18, resnet34, resnet50\n",
    "    weights: \"IMAGENET1K_V1\", None\n",
    "    \"\"\"\n",
    "    # Use standard ResNet implementation from torchvision\n",
    "    func = getattr(torchvision.models, name)\n",
    "    resnet = func(weights=weights, **kwargs)\n",
    "\n",
    "    # remove the final fully connected layer\n",
    "    # for resnet18, the output dim should be 512\n",
    "    resnet.fc = torch.nn.Identity()\n",
    "    return resnet\n",
    "\n",
    "\n",
    "def replace_submodules(\n",
    "        root_module: nn.Module,\n",
    "        predicate: Callable[[nn.Module], bool],\n",
    "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replace all submodules selected by the predicate with\n",
    "    the output of func.\n",
    "\n",
    "    predicate: Return true if the module is to be replaced.\n",
    "    func: Return new module to use.\n",
    "    \"\"\"\n",
    "    if predicate(root_module):\n",
    "        return func(root_module)\n",
    "\n",
    "    bn_list = [k.split('.') for k, m\n",
    "        in root_module.named_modules(remove_duplicate=True)\n",
    "        if predicate(m)]\n",
    "    for *parent, k in bn_list:\n",
    "        parent_module = root_module\n",
    "        if len(parent) > 0:\n",
    "            parent_module = root_module.get_submodule('.'.join(parent))\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            src_module = parent_module[int(k)]\n",
    "        else:\n",
    "            src_module = getattr(parent_module, k)\n",
    "        tgt_module = func(src_module)\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            parent_module[int(k)] = tgt_module\n",
    "        else:\n",
    "            setattr(parent_module, k, tgt_module)\n",
    "    # verify that all modules are replaced\n",
    "    bn_list = [k.split('.') for k, m\n",
    "        in root_module.named_modules(remove_duplicate=True)\n",
    "        if predicate(m)]\n",
    "    assert len(bn_list) == 0\n",
    "    return root_module\n",
    "\n",
    "def replace_bn_with_gn(\n",
    "    root_module: nn.Module,\n",
    "    features_per_group: int=16) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Relace all BatchNorm layers with GroupNorm.\n",
    "    \"\"\"\n",
    "    replace_submodules(\n",
    "        root_module=root_module,\n",
    "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
    "        func=lambda x: nn.GroupNorm(\n",
    "            num_groups=x.num_features//features_per_group,\n",
    "            num_channels=x.num_features)\n",
    "    )\n",
    "    return root_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1657,
     "status": "ok",
     "timestamp": 1703131329335,
     "user": {
      "displayName": "Chi Cheng",
      "userId": "13145723388682673807"
     },
     "user_tz": 480
    },
    "id": "4APZkqh336-M",
    "outputId": "e362d582-ceac-4cdd-e866-c34858593696",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.257856e+07\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Network Demo**\n",
    "\n",
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "vision_encoder = get_resnet('resnet18')\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "# ResNet18 has output dim of 512\n",
    "vision_feature_dim = 512\n",
    "# agent_pos is 2 dimensional\n",
    "lowdim_obs_dim = 2\n",
    "# observation feature has 514 dims in total per step\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "action_dim = 2\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vision_encoder,\n",
    "    'noise_pred_net': noise_pred_net\n",
    "})\n",
    "\n",
    "# demo\n",
    "with torch.no_grad():\n",
    "    # example inputs\n",
    "    image = torch.zeros((1, obs_horizon,3,96,96))\n",
    "    agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "    # vision encoder\n",
    "    image_features = nets['vision_encoder'](\n",
    "        image.flatten(end_dim=1))\n",
    "    # (2,512)\n",
    "    image_features = image_features.reshape(*image.shape[:2],-1)\n",
    "    # (1,2,512)\n",
    "    obs = torch.cat([image_features, agent_pos],dim=-1)\n",
    "    # (1,2,514)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "    diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "    # the noise prediction network\n",
    "    # takes noisy action, diffusion iteration and observation as input\n",
    "    # predicts the noise added to action\n",
    "    noise = nets['noise_pred_net'](\n",
    "        sample=noised_action,\n",
    "        timestep=diffusion_iter,\n",
    "        global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "    # illustration of removing noise\n",
    "    # the actual noise removal is performed by NoiseScheduler\n",
    "    # and is dependent on the diffusion noise schedule\n",
    "    denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CombinedMultiPushTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, perturb_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.perturb_dataset = perturb_dataset\n",
    "        self.base_len = len(base_dataset)\n",
    "        self.perturb_len = len(perturb_dataset)\n",
    "        self.length = max(self.base_len, self.perturb_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base = self.base_dataset[idx % self.base_len]\n",
    "        perturb = self.perturb_dataset[idx % self.perturb_len]\n",
    "        return {\n",
    "            'base_image': base['image'],\n",
    "            'base_agent_pos': base['agent_pos'],\n",
    "            'base_action': base['action'],\n",
    "            'perturb_image': perturb['image'],\n",
    "            'perturb_agent_pos': perturb['agent_pos'],\n",
    "            'perturb_action': perturb['action'],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_dataset = MultiPushTImageDataset(\n",
    "    dataset_paths=['data/range_A_base.zarr', 'data/range_B_base.zarr'],\n",
    "    pred_horizon=16,\n",
    "    obs_horizon=1,\n",
    "    action_horizon=8\n",
    ")\n",
    "\n",
    "perturb_dataset = MultiPushTImageDataset(\n",
    "    dataset_paths=['data/range_A_perturb.zarr'],\n",
    "    pred_horizon=16,\n",
    "    obs_horizon=1,\n",
    "    action_horizon=8\n",
    ")\n",
    "\n",
    "combined_dataset = CombinedMultiPushTDataset(base_dataset, perturb_dataset)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resuming training from checkpoints/ot_baseline_old/ckpt.ckpt\n",
      " Resumed from epoch 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192a8a58a06b4fa8b662c26cafeb580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5763fda6fe6d425ebe1b8749a75ecbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 97\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bc_loss \u001b[38;5;241m+\u001b[39m (OT_TEMPERATURE \u001b[38;5;241m*\u001b[39m ot_loss)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/coc/flash7/rpunamiya6/miniconda3/envs/diffp/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash7/rpunamiya6/miniconda3/envs/diffp/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash7/rpunamiya6/miniconda3/envs/diffp/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "save_dir = 'checkpoints/ot_baseline_old'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = os.path.join(save_dir, 'ckpt.ckpt')\n",
    "start_epoch = 0\n",
    "num_epochs = 110\n",
    "\n",
    "# Initialize optimizer and EMA first\n",
    "ema = EMAModel(parameters=nets.parameters(), power=0.75)\n",
    "\n",
    "ot_start_epoch = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(f\" Resuming training from {ckpt_path}\")\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    nets.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\" Resumed from epoch {start_epoch}\")\n",
    "\n",
    "ot_loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.05)\n",
    "OT_TEMPERATURE = 1.0\n",
    "with tqdm(range(start_epoch, num_epochs), desc='Epoch') as tglobal:\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = []\n",
    "        bc_epoch_loss = []\n",
    "        ot_epoch_loss = []\n",
    "        with tqdm(loader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                                # === Base domain ===\n",
    "                base_img = nbatch['base_image'][:, :obs_horizon].to(device).float() / 255.0  # [B, T, 3, 96, 96]\n",
    "                base_pos = nbatch['base_agent_pos'][:, :obs_horizon].to(device)              # [B, T, 2]\n",
    "                base_action = nbatch['base_action'].to(device)                               # [B, T, 2]\n",
    "\n",
    "                # === Perturb domain ===\n",
    "                pert_img = nbatch['perturb_image'][:, :obs_horizon].to(device).float() / 255.0\n",
    "                pert_pos = nbatch['perturb_agent_pos'][:, :obs_horizon].to(device)\n",
    "                pert_action = nbatch['perturb_action'].to(device)\n",
    "\n",
    "                B = base_img.shape[0]\n",
    "\n",
    "                # ----- Encode base -----\n",
    "                base_feat = nets['vision_encoder'](base_img.flatten(0, 1))           # [B*T, 512]\n",
    "                base_feat = base_feat.view(B, obs_horizon, -1)                       # [B, T, 512]\n",
    "                base_obs = torch.cat([base_feat, base_pos], dim=-1)                  # [B, T, 514]\n",
    "                base_obs_cond = base_obs.flatten(start_dim=1)\n",
    "\n",
    "                # ----- Encode perturb -----\n",
    "                pert_feat = nets['vision_encoder'](pert_img.flatten(0, 1))\n",
    "                pert_feat = pert_feat.view(B, obs_horizon, -1)\n",
    "                pert_obs = torch.cat([pert_feat, pert_pos], dim=-1)\n",
    "                pert_obs_cond = pert_obs.flatten(start_dim=1)\n",
    "\n",
    "                # ----- OT loss  -----\n",
    "                ot_loss = ot_loss_fn(pert_obs_cond, base_obs_cond)\n",
    "                # ----- Base diffusion -----\n",
    "                t_base = torch.randint(0, num_diffusion_iters, (B,), device=device).long()\n",
    "                noise_base = torch.randn_like(base_action)\n",
    "                noisy_base_action = noise_scheduler.add_noise(base_action, noise_base, t_base)\n",
    "                noise_pred_base = noise_pred_net(noisy_base_action, t_base, global_cond=base_obs_cond)\n",
    "                loss_base = nn.functional.mse_loss(noise_pred_base, noise_base)\n",
    "\n",
    "                # ----- Perturb diffusion -----\n",
    "                t_pert = torch.randint(0, num_diffusion_iters, (B,), device=device).long()\n",
    "                noise_pert = torch.randn_like(pert_action)\n",
    "                noisy_pert_action = noise_scheduler.add_noise(pert_action, noise_pert, t_pert)\n",
    "                noise_pred_pert = noise_pred_net(noisy_pert_action, t_pert, global_cond=pert_obs_cond)\n",
    "                loss_pert = nn.functional.mse_loss(noise_pred_pert, noise_pert)\n",
    "\n",
    "                # ----- Combined loss -----\n",
    "\n",
    "                bc_loss = loss_base + loss_pert\n",
    "                if epoch_idx < ot_start_epoch:\n",
    "                    loss = bc_loss + (0 * ot_loss)\n",
    "                else:\n",
    "                    \n",
    "                    loss = bc_loss + (OT_TEMPERATURE * ot_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "                ema.step(nets.parameters())\n",
    "\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                ot_epoch_loss.append(ot_loss.item())\n",
    "                bc_epoch_loss.append(bc_loss.item())\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        tglobal.set_postfix({\n",
    "                'loss': np.mean(epoch_loss),\n",
    "                'bc': np.mean(bc_epoch_loss),\n",
    "                'ot': np.mean(ot_epoch_loss)\n",
    "            })\n",
    "        # Save checkpoint\n",
    "        if (epoch_idx + 1) % 10 == 0 or (epoch_idx + 1) == num_epochs:\n",
    "            ckpt_epoch_path = os.path.join(save_dir, f'ckpt_epoch_{epoch_idx+1}.ckpt')\n",
    "            torch.save({\n",
    "                'epoch': epoch_idx + 1,\n",
    "                'model_state_dict': nets.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'ema_state_dict': ema.state_dict(),\n",
    "                'config': {\n",
    "                    'obs_horizon': obs_horizon,\n",
    "                    'action_dim': action_dim,\n",
    "                    'obs_dim': obs_dim,\n",
    "                    'num_diffusion_iters': num_diffusion_iters\n",
    "                }\n",
    "            }, ckpt_epoch_path)\n",
    "            torch.save({\n",
    "                'epoch': epoch_idx + 1,\n",
    "                'model_state_dict': nets.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'ema_state_dict': ema.state_dict(),\n",
    "            }, ckpt_path)\n",
    "            print(f\" Saved checkpoint to {ckpt_epoch_path} and {ckpt_path}\")\n",
    "\n",
    "# Apply EMA weights at end of training\n",
    "ema.copy_to(nets.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import SoftDTWLossPyTorch\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "def compute_ot_loss(\n",
    "    tokens1, tokens2,\n",
    "    emb1_actions, emb2_actions,\n",
    "    supervised=True,\n",
    "    lambd=2.0,\n",
    "    gamma=1.0,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes OT loss between `tokens1` and `tokens2`, optionally with SoftDTW supervision.\n",
    "    \n",
    "    Args:\n",
    "        tokens1: Tensor of shape [B, D] (base embeddings)\n",
    "        tokens2: Tensor of shape [B, D] (perturbed embeddings)\n",
    "        emb1_actions: Tensor of shape [B, T, D] (base actions)\n",
    "        emb2_actions: Tensor of shape [B, T, D] (perturbed actions)\n",
    "        supervised: Whether to use supervised alignment\n",
    "        lambd: Weight for aligned pairs in the OT cost matrix\n",
    "        gamma: SoftDTW smoothing parameter\n",
    "        device: Device to compute on (e.g., \"cuda\" or \"cpu\")\n",
    "        \n",
    "    Returns:\n",
    "        ot_loss: Scalar OT loss\n",
    "        avg_feature_dist: Average embedding feature distance\n",
    "    \"\"\"\n",
    "    B, T, D = emb1_actions.shape\n",
    "\n",
    "    if not supervised:\n",
    "        ot_loss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=0.1)\n",
    "        ot_loss = ot_loss_fn(tokens2, tokens1)\n",
    "        avg_feature_dist = torch.norm(tokens2 - tokens1, dim=-1).mean()\n",
    "        return ot_loss, avg_feature_dist\n",
    "\n",
    "    # --- Supervised using SoftDTW ---\n",
    "    dtw_loss_fn = SoftDTWLossPyTorch(gamma=gamma)\n",
    "    pairwise_dists = torch.zeros(B, B, device=device)\n",
    "    \n",
    "    emb2_delta = emb2_actions - emb2_actions[:, :1, :]\n",
    "    emb1_delta = emb1_actions - emb1_actions[:, :1, :]\n",
    "    emb2_expand = emb2_delta.unsqueeze(1).expand(B, B, -1, -1)\n",
    "    emb1_expand = emb1_delta.unsqueeze(0).expand(B, B, -1, -1)\n",
    "    pairwise_dist = dtw_loss_fn(\n",
    "        emb2_expand.reshape(B * B, *emb2_actions.shape[1:]),\n",
    "        emb1_expand.reshape(B * B, *emb1_actions.shape[1:])\n",
    "    ).view(B, B)\n",
    "\n",
    "    # Label alignment by min-DTW\n",
    "    labels = torch.argmin(pairwise_dists, dim=1)  # [B]\n",
    "\n",
    "    # Construct supervision weight matrix\n",
    "    W = torch.ones(B, B, device=device)\n",
    "    W[torch.arange(B), labels] = lambd\n",
    "\n",
    "    def make_custom_cost(W_matrix):\n",
    "        def cost_fn(x, y):\n",
    "            x_exp = x.unsqueeze(1)  # [B, 1, D]\n",
    "            y_exp = y.unsqueeze(0)  # [1, B, D]\n",
    "            cost = ((x_exp - y_exp) ** 2).sum(-1)  # [B, B]\n",
    "            return cost * W_matrix\n",
    "        return cost_fn\n",
    "\n",
    "    custom_cost = make_custom_cost(W)\n",
    "    ot_loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.05, truncate=18, cost=custom_cost)\n",
    "    ot_loss = ot_loss_fn(tokens2, tokens1)\n",
    "    avg_feature_dist = torch.norm(tokens2 - tokens1, dim=-1).mean()\n",
    "\n",
    "    return ot_loss, avg_feature_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "save_dir = 'checkpoints/scaled_ot'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = os.path.join(save_dir, 'ckpt.ckpt')\n",
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize optimizer and EMA first\n",
    "ema = EMAModel(parameters=nets.parameters(), power=0.75)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(f\" Resuming training from {ckpt_path}\")\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    nets.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\" Resumed from epoch {start_epoch}\")\n",
    "\n",
    "ot_loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.1)\n",
    "OT_TEMPERATURE = 0.7\n",
    "with tqdm(range(start_epoch, num_epochs), desc='Epoch') as tglobal:\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = []\n",
    "        bc_epoch_loss = []\n",
    "        ot_epoch_loss = []\n",
    "        with tqdm(loader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                                # === Base domain ===\n",
    "                base_img = nbatch['base_image'][:, :obs_horizon].to(device).float() / 255.0  # [B, T, 3, 96, 96]\n",
    "                base_pos = nbatch['base_agent_pos'][:, :obs_horizon].to(device)              # [B, T, 2]\n",
    "                base_action = nbatch['base_action'].to(device)                               # [B, T, 2]\n",
    "\n",
    "                # === Perturb domain ===\n",
    "                pert_img = nbatch['perturb_image'][:, :obs_horizon].to(device).float() / 255.0\n",
    "                pert_pos = nbatch['perturb_agent_pos'][:, :obs_horizon].to(device)\n",
    "                pert_action = nbatch['perturb_action'].to(device)\n",
    "\n",
    "                B = base_img.shape[0]\n",
    "\n",
    "                # ----- Encode base -----\n",
    "                base_feat = nets['vision_encoder'](base_img.flatten(0, 1))           # [B*T, 512]\n",
    "                base_feat = base_feat.view(B, obs_horizon, -1)                       # [B, T, 512]\n",
    "                base_obs = torch.cat([base_feat, base_pos], dim=-1)                  # [B, T, 514]\n",
    "                base_obs_cond = base_obs.flatten(start_dim=1)\n",
    "\n",
    "                # ----- Encode perturb -----\n",
    "                pert_feat = nets['vision_encoder'](pert_img.flatten(0, 1))\n",
    "                pert_feat = pert_feat.view(B, obs_horizon, -1)\n",
    "                pert_obs = torch.cat([pert_feat, pert_pos], dim=-1)\n",
    "                pert_obs_cond = pert_obs.flatten(start_dim=1)\n",
    "\n",
    "                # ----- OT loss  -----\n",
    "                ot_loss = ot_loss, avg_feat_dist = compute_ot_loss(\n",
    "                                tokens1=pert_obs_cond,\n",
    "                                tokens2=base_obs_cond,\n",
    "                                emb1_actions=pert_action,\n",
    "                                emb2_actions=base_action,\n",
    "                                supervised=True,\n",
    "                                lambd=0.5,\n",
    "                                gamma=0.1,\n",
    "                                device=device)\n",
    "                # ----- Base diffusion -----\n",
    "                t_base = torch.randint(0, num_diffusion_iters, (B,), device=device).long()\n",
    "                noise_base = torch.randn_like(base_action)\n",
    "                noisy_base_action = noise_scheduler.add_noise(base_action, noise_base, t_base)\n",
    "                noise_pred_base = noise_pred_net(noisy_base_action, t_base, global_cond=base_obs_cond)\n",
    "                loss_base = nn.functional.mse_loss(noise_pred_base, noise_base)\n",
    "\n",
    "                # ----- Perturb diffusion -----\n",
    "                t_pert = torch.randint(0, num_diffusion_iters, (B,), device=device).long()\n",
    "                noise_pert = torch.randn_like(pert_action)\n",
    "                noisy_pert_action = noise_scheduler.add_noise(pert_action, noise_pert, t_pert)\n",
    "                noise_pred_pert = noise_pred_net(noisy_pert_action, t_pert, global_cond=pert_obs_cond)\n",
    "                loss_pert = nn.functional.mse_loss(noise_pred_pert, noise_pert)\n",
    "\n",
    "                # ----- Combined loss -----\n",
    "                bc_loss = loss_base + loss_pert\n",
    "                loss = bc_loss + (OT_TEMPERATURE * ot_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "                ema.step(nets.parameters())\n",
    "\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                ot_epoch_loss.append(ot_loss.item())\n",
    "                bc_epoch_loss.append(bc_loss.item())\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        tglobal.set_postfix({\n",
    "                'loss': np.mean(epoch_loss),\n",
    "                'bc': np.mean(bc_epoch_loss),\n",
    "                'ot': np.mean(ot_epoch_loss)\n",
    "            })\n",
    "        # Save checkpoint\n",
    "        if (epoch_idx + 1) % 10 == 0 or (epoch_idx + 1) == num_epochs:\n",
    "            ckpt_epoch_path = os.path.join(save_dir, f'ckpt_epoch_{epoch_idx+1}.ckpt')\n",
    "            torch.save({\n",
    "                'epoch': epoch_idx + 1,\n",
    "                'model_state_dict': nets.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'ema_state_dict': ema.state_dict(),\n",
    "                'config': {\n",
    "                    'obs_horizon': obs_horizon,\n",
    "                    'action_dim': action_dim,\n",
    "                    'obs_dim': obs_dim,\n",
    "                    'num_diffusion_iters': num_diffusion_iters\n",
    "                }\n",
    "            }, ckpt_epoch_path)\n",
    "            torch.save({\n",
    "                'epoch': epoch_idx + 1,\n",
    "                'model_state_dict': nets.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'ema_state_dict': ema.state_dict(),\n",
    "            }, ckpt_path)\n",
    "            print(f\" Saved checkpoint to {ckpt_epoch_path} and {ckpt_path}\")\n",
    "\n",
    "# Apply EMA weights at end of training\n",
    "ema.copy_to(nets.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13013,
     "status": "ok",
     "timestamp": 1703131388601,
     "user": {
      "displayName": "Chi Cheng",
      "userId": "13145723388682673807"
     },
     "user_tz": 480
    },
    "id": "6F3hUbIuxGdO",
    "outputId": "36fa4685-4c4a-4ef3-a0a5-add134288f88",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded.\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Loading Pretrained Checkpoint**\n",
    "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
    "\n",
    "load_pretrained = True\n",
    "if load_pretrained:\n",
    "  ckpt_path = \"checkpoints/ot_baseline/ckpt_epoch_100.ckpt\"\n",
    "  if not os.path.isfile(ckpt_path):\n",
    "      id = \"1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\"\n",
    "     # gdown.download(id=id, output=ckpt_path, quiet=False)\n",
    "\n",
    "  state_dict = torch.load(ckpt_path, map_location='cuda')[\"model_state_dict\"]\n",
    "  ema_nets = nets\n",
    "  ema_nets.load_state_dict(state_dict)\n",
    "  print('Pretrained weights loaded.')\n",
    "else:\n",
    "  print(\"Skipped pretrained weight loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 35353,
     "status": "ok",
     "timestamp": 1703131426802,
     "user": {
      "displayName": "Chi Cheng",
      "userId": "13145723388682673807"
     },
     "user_tz": 480
    },
    "id": "OyLjlNQk5nr9",
    "outputId": "4641c055-a534-48e2-cc26-0d1056a004ba",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d92a0345ef448b4b08efc1285a1e2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval PushTImageEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "### In-domain\n",
    "# limit enviornment interaction to 200 steps before termination\n",
    "\n",
    "shared_stats = base_dataset.shared_stats\n",
    "\n",
    "max_steps = 200\n",
    "env = PushTImageEnv()\n",
    "# use a seed >200 to avoid initial states seen in the training dataset\n",
    "env.seed(100)\n",
    "\n",
    "# get first observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "# keep a queue of last 2 steps of observations\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode='rgb_array')]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "stats = shared_stats\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon number of observations\n",
    "        images = np.stack([x['image'] for x in obs_deque])\n",
    "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "        # normalize observation\n",
    "        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
    "        # images are already normalized to [0,1]\n",
    "        nimages = images\n",
    "\n",
    "        # device transfer\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "\n",
    "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "        # (2,2)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # get image features\n",
    "            image_features = ema_nets['vision_encoder'](nimages)\n",
    "            # (2,512)\n",
    "\n",
    "            # concat with low-dim observations\n",
    "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
    "\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_nets['noise_pred_net'](\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end,:]\n",
    "        # (action_horizon, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _, info = env.step(action[i])\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "# print out the maximum target coverage\n",
    "print('Score: ', max(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "# --- Config ---\n",
    "num_trials = 100\n",
    "seed_range = (101, 9999)\n",
    "max_steps = 200\n",
    "success_threshold = 0.9\n",
    "log_file = \"per_seed_scores.json\"\n",
    "\n",
    "# --- Fix global randomness ---\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Sample random but reproducible set of seeds\n",
    "eval_seeds = random.sample(range(*seed_range), num_trials)\n",
    "\n",
    "# Get shared stats from dataset\n",
    "shared_stats = base_dataset.shared_stats\n",
    "\n",
    "# --- Storage for per-seed results ---\n",
    "per_seed_scores = {}\n",
    "\n",
    "for seed in tqdm(eval_seeds, desc=\"Evaluating random seeds\"):\n",
    "    env = PushTImageEnv()\n",
    "    env.seed(seed)\n",
    "    obs, info = env.reset()\n",
    "    obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "    rewards = []\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "\n",
    "    while not done:\n",
    "        B = 1\n",
    "        images = np.stack([x['image'] for x in obs_deque])\n",
    "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "        nagent_poses = normalize_data(agent_poses, stats=shared_stats['agent_pos'])\n",
    "        nimages = images\n",
    "\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = ema_nets['vision_encoder'](nimages)\n",
    "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            noisy_action = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                noise_pred = ema_nets['noise_pred_net'](\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        naction = naction[0].cpu().numpy()\n",
    "        action_pred = unnormalize_data(naction, stats=shared_stats['action'])\n",
    "        action = action_pred[obs_horizon - 1: obs_horizon - 1 + action_horizon]\n",
    "\n",
    "        for a in action:\n",
    "            obs, reward, done, _, info = env.step(a)\n",
    "            obs_deque.append(obs)\n",
    "            rewards.append(reward)\n",
    "            step_idx += 1\n",
    "            if step_idx >= max_steps or done:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "    per_seed_scores[seed] = float(max(rewards))\n",
    "\n",
    "# --- Save to JSON file ---\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(per_seed_scores, f, indent=2)\n",
    "print(f\" Saved per-seed scores to {log_file}\")\n",
    "\n",
    "# --- Summary stats ---\n",
    "all_scores = np.array(list(per_seed_scores.values()))\n",
    "mean_reward = np.mean(all_scores)\n",
    "var_reward = np.var(all_scores)\n",
    "success_rate = np.mean(all_scores >= success_threshold)\n",
    "\n",
    "print(f\" Mean max reward: {mean_reward:.4f}\")\n",
    "print(f\" Variance: {var_reward:.4f}\")\n",
    "print(f\" Success Rate ( {success_threshold}): {success_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=640 controls>\n",
       "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "import tempfile\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Save imgs (a list of HWC RGB arrays) as mp4\n",
    "with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n",
    "    video_path = f.name\n",
    "    writer = imageio.get_writer(video_path, fps=10, format='ffmpeg')\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "\n",
    "# Display the video inline in a notebook\n",
    "with open(video_path, \"rb\") as f:\n",
    "    mp4 = f.read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=640 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6923ab86c804f9391e62c8df81103bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval PushTImageEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.055314196389569505\n"
     ]
    }
   ],
   "source": [
    "### Out of domain\n",
    "# limit enviornment interaction to 200 steps before termination\n",
    "\n",
    "shared_stats = perturb_dataset.shared_stats\n",
    "\n",
    "max_steps = 200\n",
    "env = PushTImageEnv2()\n",
    "# use a seed >200 to avoid initial states seen in the training dataset\n",
    "env.seed(2388)\n",
    "\n",
    "# get first observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "# keep a queue of last 2 steps of observations\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode='rgb_array')]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "print\n",
    "stats = shared_stats\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon number of observations\n",
    "        images = np.stack([x['image'] for x in obs_deque])\n",
    "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "        # normalize observation\n",
    "        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
    "        # images are already normalized to [0,1]\n",
    "        nimages = images\n",
    "\n",
    "        # device transfer\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "\n",
    "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "        # (2,2)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # get image features\n",
    "            image_features = ema_nets['vision_encoder'](nimages)\n",
    "            # (2,512)\n",
    "\n",
    "            # concat with low-dim observations\n",
    "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
    "\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_nets['noise_pred_net'](\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end,:]\n",
    "        # (action_horizon, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _, info = env.step(action[i])\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "# print out the maximum target coverage\n",
    "print('Score: ', max(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Fix random seed for reproducibility\u001b[39;00m\n\u001b[1;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Sample random seeds\u001b[39;00m\n\u001b[1;32m     13\u001b[0m eval_seeds \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39mseed_range), num_trials)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Config ---\n",
    "num_trials = 100\n",
    "seed_range = (101, 9999)\n",
    "max_steps = 200\n",
    "success_threshold = 0.9\n",
    "log_file = \"ood_per_seed_scores.json\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Sample random seeds\n",
    "eval_seeds = random.sample(range(*seed_range), num_trials)\n",
    "\n",
    "# Shared normalization stats from perturb dataset\n",
    "shared_stats = perturb_dataset.shared_stats\n",
    "\n",
    "# Track scores\n",
    "per_seed_scores = {}\n",
    "\n",
    "# Evaluation loop\n",
    "for seed in tqdm(eval_seeds, desc=\"Evaluating OOD seeds\"):\n",
    "    env = PushTImageEnv2()\n",
    "    env.seed(seed)\n",
    "    obs, info = env.reset()\n",
    "    obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "    rewards = []\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "\n",
    "    while not done:\n",
    "        B = 1\n",
    "        images = np.stack([x['image'] for x in obs_deque])\n",
    "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "        nagent_poses = normalize_data(agent_poses, stats=shared_stats['agent_pos'])\n",
    "        nimages = images\n",
    "\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = ema_nets['vision_encoder'](nimages)\n",
    "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            noisy_action = torch.randn((B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                noise_pred = ema_nets['noise_pred_net'](\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        naction = naction[0].cpu().numpy()\n",
    "        action_pred = unnormalize_data(naction, stats=shared_stats['action'])\n",
    "        action = action_pred[obs_horizon - 1: obs_horizon - 1 + action_horizon]\n",
    "\n",
    "        for a in action:\n",
    "            obs, reward, done, _, info = env.step(a)\n",
    "            obs_deque.append(obs)\n",
    "            rewards.append(reward)\n",
    "            step_idx += 1\n",
    "            if step_idx >= max_steps or done:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "    per_seed_scores[seed] = float(max(rewards))\n",
    "\n",
    "# Save scores to file\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(per_seed_scores, f, indent=2)\n",
    "print(f\" Saved per-seed OOD scores to {log_file}\")\n",
    "\n",
    "# Final stats\n",
    "all_scores = np.array(list(per_seed_scores.values()))\n",
    "mean_reward = np.mean(all_scores)\n",
    "var_reward = np.var(all_scores)\n",
    "success_rate = np.mean(all_scores >= success_threshold)\n",
    "\n",
    "print(f\" OOD Mean max reward: {mean_reward:.4f}\")\n",
    "print(f\" OOD Variance: {var_reward:.4f}\")\n",
    "print(f\" OOD Success Rate ( {success_threshold}): {success_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=640 controls>\n",
       "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "import tempfile\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Save imgs (a list of HWC RGB arrays) as mp4\n",
    "with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n",
    "    video_path = f.name\n",
    "    writer = imageio.get_writer(video_path, fps=10, format='ffmpeg')\n",
    "    for frame in imgs:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "\n",
    "# Display the video inline in a notebook\n",
    "with open(video_path, \"rb\") as f:\n",
    "    mp4 = f.read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=640 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg",
     "timestamp": 1743615951827
    }
   ]
  },
  "gpuClass": "standard",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7041446,
     "sourceId": 11265175,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7042841,
     "sourceId": 11266998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7043649,
     "sourceId": 11268141,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "diffp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0873a2dee0e44b0fb3e8445d94c27171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fd765bcd2a34db49a02839ba1c4f518",
      "placeholder": "",
      "style": "IPY_MODEL_d9054c34284045eda3546a21bb5fafc3",
      "value": "Epoch:   2%"
     }
    },
    "09152dbe3c8543aa809aa592afdc53f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47b6383ed3b64b7e8c7e66d0e7a468d9",
      "placeholder": "",
      "style": "IPY_MODEL_1cdde5afac8041bf9b75e0e80bd80630",
      "value": " 2/100 [03:14&lt;2:38:57, 97.32s/it, loss=0.0192]"
     }
    },
    "12ef3c026774405cb91faf18d7fd8afa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb23edb22d624aee9fd55ab05e2a517e",
      "placeholder": "",
      "style": "IPY_MODEL_a073d659b0f347c9b30c343430d7f6aa",
      "value": " 379/379 [01:37&lt;00:00,  4.40it/s, loss=0.0185]"
     }
    },
    "16b5928585984a1b81b809717f125489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1faac530b70b41a58a8c9c0cf44af69c",
      "placeholder": "",
      "style": "IPY_MODEL_7cfefcf49a9a426da4e9203ec4debe38",
      "value": " 379/379 [01:37&lt;00:00,  4.57it/s, loss=0.0103]"
     }
    },
    "182afbf676cd4c0982b90890bbdbeeef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "185ce2207cec4ae1a6ffdecadae23894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b96a78d8b8542eb830e3c446a0dcf42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cdde5afac8041bf9b75e0e80bd80630": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d0a57ed8e914d31bcc1c36a56451df7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc800669dfd45a784864dd4a3881daa",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_95dc6c44f97e4e8882a3657bd2fd66fb",
      "value": 200
     }
    },
    "1faac530b70b41a58a8c9c0cf44af69c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c31ffecd0494e2a8d0a8cefbea5df8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d4a78f691ec4555bf179e5ef5d828ac",
      "placeholder": "",
      "style": "IPY_MODEL_1b96a78d8b8542eb830e3c446a0dcf42",
      "value": "Eval PushTImageEnv: "
     }
    },
    "333b5148e59e47f3998a5cf5df531baf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353cb60a2a73417b81ccbb5e52480ed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c0a27b6addb4c6a977824995548fe90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e503eeff5d94f0e9dc8a3d31190c6a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "410476fa4ac14dd2b78377f4d779402f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a92fc95c6a3d496997da90c87e24ba01",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a98272475d940b1b98438c4d02dcc05",
      "value": 2
     }
    },
    "44c486fa8c4241f4a1a245f4e24da769": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c0a27b6addb4c6a977824995548fe90",
      "max": 379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d14f2c65ef9348348250af2df0f32963",
      "value": 379
     }
    },
    "479d2f35498b4fbea77057f3f26fd287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c25264289e3411db29f6f8db936b00f",
      "max": 379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0ff8cd1b00544caa12cbd809524dd15",
      "value": 379
     }
    },
    "47b6383ed3b64b7e8c7e66d0e7a468d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4abc5a8b257240b99535a6abe5e00ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cc800669dfd45a784864dd4a3881daa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5124950790874225b5cca7556f122f9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c1c61cd14fb4aa8b335684fbf652a29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69f323a406004822bf1a0bfd79767c9a",
      "max": 379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88a8f5924a1e4a86805e314dd90c17be",
      "value": 11
     }
    },
    "5d4a78f691ec4555bf179e5ef5d828ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f96241543b441fcb13c16d4ca476405": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6bbb6ce052045ca895b526c620d8847",
       "IPY_MODEL_5c1c61cd14fb4aa8b335684fbf652a29",
       "IPY_MODEL_a5a2bc5dc0b54a4796e574308bb6c4bc"
      ],
      "layout": "IPY_MODEL_febe66177c604395a78be993eeff7c6d"
     }
    },
    "69f323a406004822bf1a0bfd79767c9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c25264289e3411db29f6f8db936b00f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de3445a866442aa9cc7d855cb0d0740": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b4b657d68648d9be98c099d061cd5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c0237e38cfd4143aa423dea26637965",
       "IPY_MODEL_44c486fa8c4241f4a1a245f4e24da769",
       "IPY_MODEL_12ef3c026774405cb91faf18d7fd8afa"
      ],
      "layout": "IPY_MODEL_99c0577d549c4dd4a61d9ec0cf6254d5"
     }
    },
    "7508fbad995648a39d2a55953d000786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9009dc5a656c4befbf513f2574df5a7d",
       "IPY_MODEL_479d2f35498b4fbea77057f3f26fd287",
       "IPY_MODEL_16b5928585984a1b81b809717f125489"
      ],
      "layout": "IPY_MODEL_9d436c7ee90349a5966f025d095dd0cc"
     }
    },
    "7cfefcf49a9a426da4e9203ec4debe38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fd765bcd2a34db49a02839ba1c4f518": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a8f5924a1e4a86805e314dd90c17be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8c0237e38cfd4143aa423dea26637965": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5124950790874225b5cca7556f122f9a",
      "placeholder": "",
      "style": "IPY_MODEL_f609505b750747dcaff8a950131743e9",
      "value": "Batch: 100%"
     }
    },
    "9009dc5a656c4befbf513f2574df5a7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e503eeff5d94f0e9dc8a3d31190c6a2",
      "placeholder": "",
      "style": "IPY_MODEL_4abc5a8b257240b99535a6abe5e00ef6",
      "value": "Batch: 100%"
     }
    },
    "95dc6c44f97e4e8882a3657bd2fd66fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99c0577d549c4dd4a61d9ec0cf6254d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "9a98272475d940b1b98438c4d02dcc05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ada44d21f234712ac5df797c730495a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d436c7ee90349a5966f025d095dd0cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "a073d659b0f347c9b30c343430d7f6aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ff8cd1b00544caa12cbd809524dd15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a5a2bc5dc0b54a4796e574308bb6c4bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ada44d21f234712ac5df797c730495a",
      "placeholder": "",
      "style": "IPY_MODEL_c777ac64366c40ea8a1f63408879c2ab",
      "value": " 11/379 [00:03&lt;01:34,  3.91it/s, loss=0.0256]"
     }
    },
    "a92fc95c6a3d496997da90c87e24ba01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0a9f64f47d34769a47e165291d2c550": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c31ffecd0494e2a8d0a8cefbea5df8a",
       "IPY_MODEL_1d0a57ed8e914d31bcc1c36a56451df7",
       "IPY_MODEL_e1bb86eb510b47d9ac2311926455bf1f"
      ],
      "layout": "IPY_MODEL_182afbf676cd4c0982b90890bbdbeeef"
     }
    },
    "b34bf5a5d603446fb453bbed31c4469c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0873a2dee0e44b0fb3e8445d94c27171",
       "IPY_MODEL_410476fa4ac14dd2b78377f4d779402f",
       "IPY_MODEL_09152dbe3c8543aa809aa592afdc53f1"
      ],
      "layout": "IPY_MODEL_e2a91fa6d1514913892c0def2e2ecf78"
     }
    },
    "c777ac64366c40ea8a1f63408879c2ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d14f2c65ef9348348250af2df0f32963": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9054c34284045eda3546a21bb5fafc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1bb86eb510b47d9ac2311926455bf1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_333b5148e59e47f3998a5cf5df531baf",
      "placeholder": "",
      "style": "IPY_MODEL_185ce2207cec4ae1a6ffdecadae23894",
      "value": " 201/? [00:34&lt;00:00,  6.11it/s, reward=0.828]"
     }
    },
    "e2a91fa6d1514913892c0def2e2ecf78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6bbb6ce052045ca895b526c620d8847": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6de3445a866442aa9cc7d855cb0d0740",
      "placeholder": "",
      "style": "IPY_MODEL_353cb60a2a73417b81ccbb5e52480ed3",
      "value": "Batch:   3%"
     }
    },
    "eb23edb22d624aee9fd55ab05e2a517e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f609505b750747dcaff8a950131743e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "febe66177c604395a78be993eeff7c6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
